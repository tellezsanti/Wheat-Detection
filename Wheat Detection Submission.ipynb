{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import os\n\nimport numpy as np\nimport pandas as pd\n\nfrom PIL import Image\n\nimport torch\nimport torchvision\n\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom torchvision.models.detection import FasterRCNN\nfrom torchvision.models.detection.rpn import AnchorGenerator\n\nimport albumentations as A\nfrom albumentations.pytorch.transforms import ToTensorV2\n\nfrom torch.utils.data import DataLoader, Dataset","execution_count":12,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"TEST_DIR = \"../input/global-wheat-detection/test/\"\nTEST_CSV_DIR = \"../input/global-wheat-detection/sample_submission.csv\"\nWEIGHTS = \"../input/santi-model-2/santi_2.pth\"","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df = pd.read_csv(TEST_CSV_DIR)","execution_count":3,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"class WheatTestDataset(Dataset):\n\n    def __init__(self, root, bboxes, transforms=None):\n        super().__init__()\n\n        self.imgs = bboxes['image_id'].unique()\n        self.bboxes = bboxes\n        self.root = root\n        self.transforms = transforms\n\n    def __getitem__(self, idx):\n\n        img_id = self.imgs[idx]\n        bbox = self.bboxes[self.bboxes['image_id'] == img_id]\n        \n        img_path = os.path.join(self.root, img_id + \".jpg\")\n        img = np.array(Image.open(img_path).convert(\"RGB\")) / 255\n\n        if self.transforms:\n            sample = {\n                'image': img,\n            }\n            sample = self.transforms(**sample)\n            img = sample['image']\n        \n        img = torch.as_tensor(img, dtype=torch.float32)\n\n        return img, img_id\n\n    def __len__(self):\n        return self.imgs.shape[0]","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Albumentations\ndef get_test_transform():\n    return A.Compose([\n        # A.Resize(512, 512),\n        ToTensorV2(p=1.0)\n    ])","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# load a model; pre-trained on COCO\nmodel = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=False, pretrained_backbone=False)\n\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n\nnum_classes = 2  # 1 class (wheat) + background\n\n# get number of input features for the classifier\nin_features = model.roi_heads.box_predictor.cls_score.in_features\n\n# replace the pre-trained head with a new one\nmodel.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n\n# Load the trained weights\nmodel.load_state_dict(torch.load(WEIGHTS))\nmodel.eval()\n\nx = model.to(device)","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def collate_fn(batch):\n    return tuple(zip(*batch))\n\ntest_dataset = WheatTestDataset(TEST_DIR, test_df, get_test_transform())\n\ntest_data_loader = DataLoader(\n    test_dataset,\n    batch_size=4,\n    shuffle=False,\n    num_workers=4,\n    drop_last=False,\n    collate_fn=collate_fn\n)","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def format_prediction_string(boxes, scores):\n    pred_strings = []\n    for j in zip(scores, boxes):\n        pred_strings.append(\"{0:.4f} {1} {2} {3} {4}\".format(j[0], j[1][0], j[1][1], j[1][2], j[1][3]))\n\n    return \" \".join(pred_strings)","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"detection_threshold = 0.5\nresults = []\n\nfor images, image_ids in test_data_loader:\n\n    images = list(image.to(device) for image in images)\n    outputs = model(images)\n\n    for i, image in enumerate(images):\n\n        boxes = outputs[i]['boxes'].data.cpu().numpy()\n        scores = outputs[i]['scores'].data.cpu().numpy()\n        \n        boxes = boxes[scores >= detection_threshold].astype(np.int32)\n        scores = scores[scores >= detection_threshold]\n        image_id = image_ids[i]\n        \n        boxes[:, 2] = boxes[:, 2] - boxes[:, 0]\n        boxes[:, 3] = boxes[:, 3] - boxes[:, 1]\n        \n        result = {\n            'image_id': image_id,\n            'PredictionString': format_prediction_string(boxes, scores)\n        }\n\n        \n        results.append(result)","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df = pd.DataFrame(results, columns=['image_id', 'PredictionString'])\ntest_df.head()","execution_count":14,"outputs":[{"output_type":"execute_result","execution_count":14,"data":{"text/plain":"    image_id                                   PredictionString\n0  aac893a91  0.9825 614 916 80 108 0.9505 552 523 129 202 0...\n1  51f1be19e  0.9773 608 77 166 190 0.9718 497 462 221 118 0...\n2  f5a1f0358  0.9779 138 750 160 124 0.9776 283 453 170 112 ...\n3  796707dd7  0.9630 892 332 118 95 0.7622 48 82 158 125 0.7...\n4  51b3e36ab  0.9949 231 644 95 161 0.9927 0 436 106 347 0.9...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_id</th>\n      <th>PredictionString</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>aac893a91</td>\n      <td>0.9825 614 916 80 108 0.9505 552 523 129 202 0...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>51f1be19e</td>\n      <td>0.9773 608 77 166 190 0.9718 497 462 221 118 0...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>f5a1f0358</td>\n      <td>0.9779 138 750 160 124 0.9776 283 453 170 112 ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>796707dd7</td>\n      <td>0.9630 892 332 118 95 0.7622 48 82 158 125 0.7...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>51b3e36ab</td>\n      <td>0.9949 231 644 95 161 0.9927 0 436 106 347 0.9...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.to_csv('submission.csv', index=False)","execution_count":15,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}